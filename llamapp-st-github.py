# RAGã‚’è©¦é¨“çš„ã«å®Ÿè£…ã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã€‚GitHubç”¨

import os
import streamlit as st
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms import LlamaCpp
from langchain.schema.runnable import RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings
from chromadb import EphemeralClient

model_name = "modelsãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ›"
model_path = "models/" + model_name

# ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ã‚’ç¢ºèª
if not os.path.exists(model_path): 
    raise FileNotFoundError(f"Model file not found: {model_path}")

# ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
embed_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L3-v2")

# ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã®è¨­å®š
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)

# Chromaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
client = EphemeralClient()

def reset_rag_components():
    """
    RAGã®é–¢é€£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹é–¢æ•°
    - vectorstore
    - rag_chain
    ã‚’Noneã«è¨­å®šã—ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™
    """
    st.session_state.vectorstore = None
    st.session_state.rag_chain = None
    global client #ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ä½¿ç”¨
    # æ–°ã—ã„Chromaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆï¼ˆæ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼‰
    client = EphemeralClient()

    # æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã—ã¦å‰Šé™¤ï¼ˆã“ã‚ŒãŒãªã„ã¨ãƒªã‚»ãƒƒãƒˆã•ã‚Œãªã„ï¼‰
    collections = client.list_collections()
    for collection in collections:
        client.delete_collection(collection.name)
    
    return "ðŸ’¡ RAGãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã—ãŸ"

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¨RAGãƒã‚§ãƒ¼ãƒ³ã®åˆæœŸåŒ–
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
prompt_template = ChatPromptTemplate.from_template("""
    äººé–“: ä»¥ä¸‹ã®å†…å®¹ã«åŸºã¥ã„ã¦è³ªå•ã«æ—¥æœ¬èªžã§ç­”ãˆã¦ãã ã•ã„ã€‚
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}
    è³ªå•: {question}
    AI: ã‚ãªãŸã¯ç´ æ™´ã‚‰ã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦å›žç­”ã—ã¾ã™ã€‚
    """)

CONTEXT_SIZE = 4096

# LLMã®è¨­å®š
if 'llm' not in st.session_state:
    st.session_state.llm = LlamaCpp(
        model_path=model_path,
        n_threads=os.cpu_count(),
        n_batch=512,
        f16_kv=True,
        verbose=False,
        n_ctx=CONTEXT_SIZE,
        temperature=0.7,
        top_p=0.95,
        repeat_penalty=1.1,
        use_mmap=True,
    )

def get_llama_response(prompt):
    return st.session_state.llm.stream(prompt, max_tokens=4096)

def create_vectorstore(text):
    texts = text_splitter.split_text(text)
    metadatas = [{"source": f"doc_{i}"} for i in range(len(texts))]
    vectorstore = Chroma.from_texts(texts, embed_model, metadatas=metadatas, client=client)
    return vectorstore

# Chromaãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã®è¨­å®š
def get_retriever(vectorstore):
    return vectorstore.as_retriever(search_kwargs={"k": 3})

# RAGãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
def build_rag_chain(retriever):
    chain = (
        {"context": retriever, "question": RunnablePassthrough()} 
        | prompt_template 
        | st.session_state.llm
    )
    return chain

# ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†ã‚’è¡Œã†é–¢æ•°
def process_uploaded_file(uploaded_file):
    try:
        text_content = uploaded_file.read().decode("utf-8")
    except UnicodeDecodeError:
        uploaded_file.seek(0)
        try:
            text_content = uploaded_file.read().decode("cp932")
        except UnicodeDecodeError:
            st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã€‚UTF-8ã¾ãŸã¯cp932ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
            return None
    return text_content

#############################################################################

#ã€€ã‚¿ã‚¤ãƒˆãƒ«
st.subheader("ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆÎ±ã€Chat-Î±ã€‘")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ä½œè€…æƒ…å ±ã‚’è¿½åŠ 
st.markdown("---")
st.sidebar.markdown("---")

# Expanderã‚’ä½¿ç”¨ã—ã¦è¿½åŠ æƒ…å ±ã‚’æä¾›
with st.expander("ã“ã®ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦"):
    st.write("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã—ãŸæ–‡ç« ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã«ç­”ãˆã‚‹ç°¡æ˜“çš„ãªãƒãƒ£ãƒƒãƒˆãƒ„ãƒ¼ãƒ«ã§ã™ã€‚æœ€ä¸‹æ®µã®ãƒ•ã‚©ãƒ¼ãƒ ã«æ–‡å­—ã‚’å…¥åŠ›ã—ã¦ã€è©±ã—æŽ›ã‘ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")
    st.write(f"è¨€èªžãƒ¢ãƒ‡ãƒ«: {model_path.lstrip("models/")}")
    
# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å‡¦ç†ã¨RAG
uploaded_file = st.sidebar.file_uploader("ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆRAGï¼‰", type = ["txt"], key = "RAG-input", )
if uploaded_file is not None:
    text_content = process_uploaded_file(uploaded_file)
    if text_content:
        st.session_state.vectorstore = create_vectorstore(text_content)
        retriever = get_retriever(st.session_state.vectorstore)
        st.session_state.rag_chain = build_rag_chain(retriever)
        st.sidebar.success("ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€å‡¦ç†ã•ã‚Œã¾ã—ãŸã€‚")
    else:
        st.sidebar.error("ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# RAGå‡¦ç†ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢
txt = st.sidebar.text_area(
    "ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’RAGã§è¿½åŠ ", 
    "",
    placeholder="ã“ã“ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
    )

# æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®è¿½åŠ 
if txt:
    st.sidebar.caption(f"æ–‡å­—æ•°: {len(txt)}")

if txt and txt.strip(): #ç©ºç™½æ–‡å­—åˆ—ãªã©ã‚’ç„¡åŠ¹ãªå…¥åŠ›ã¨ã—ã¦æ‰±ã†
    st.session_state.vectorstore = create_vectorstore(txt)
    retriever = get_retriever(st.session_state.vectorstore)
    st.session_state.rag_chain = build_rag_chain(retriever)
    st.sidebar.success("ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€å‡¦ç†ã•ã‚Œã¾ã—ãŸã€‚")

# å‡¦ç†çŠ¶æ…‹ã®è¡¨ç¤º
if st.session_state.vectorstore is not None:
    st.sidebar.success("RAGã‚¢ã‚¯ãƒ†ã‚£ãƒ–")
else:
    st.sidebar.warning("RAGæœªè¨­å®š")

# åˆæœŸåŒ–ãƒœã‚¿ãƒ³
if st.sidebar.button("RAGã‚’ãƒªã‚»ãƒƒãƒˆ"):
    status_message = reset_rag_components()
    st.sidebar.info(status_message)
    st.rerun()

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‡¦ç†
if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ãƒœãƒƒãƒˆã®å¿œç­”ã‚’ç”Ÿæˆ
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        response = get_llama_response(prompt)

        if st.session_state.rag_chain:
            for chunk in st.session_state.rag_chain.stream(prompt):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
        else:
            full_response = ""
            for chunk in response:
                if isinstance(chunk, dict) and 'choices' in chunk and len(chunk['choices']) > 0:
                    content = chunk['choices'][0]['text']
                    full_response += content
                    message_placeholder.markdown(full_response + "â–Œ")
                else:
                    full_response += chunk
                    message_placeholder.markdown(full_response + "â–Œ")
        
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})